{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "NgL6cIzNBDul",
      "metadata": {
        "id": "NgL6cIzNBDul"
      },
      "source": [
        "**Title: Balanced Dataset Creation for Binary Classification**\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "The data sampling process involved creating a balanced dataset from an imbalanced source dataset containing binary classifications (Yes/No). The original dataset showed significant class imbalance with 9,728 \"Yes\" samples and 1,433 \"No\" samples. To address this imbalance, random sampling was performed on the majority class (\"Yes\") to match the size of the minority class (\"No\"). The sampling was conducted using a fixed random seed (42) to ensure reproducibility. After sampling, the data was shuffled to prevent any order-based biases.\n",
        "\n",
        "**Output:**\n",
        "\n",
        "The sampling process resulted in a perfectly balanced dataset with the following characteristics:\n",
        "\n",
        "Final size: 2,866 total samples\n",
        "\n",
        "\"Yes\" class: 1,433 samples\n",
        "\n",
        "\"No\" class: 1,433 samples\n",
        "\n",
        "The balanced dataset was saved as 'balanced_dataset.csv' for subsequent model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5aed690-410b-44f1-bdfa-686e5b72ebe3",
      "metadata": {
        "id": "e5aed690-410b-44f1-bdfa-686e5b72ebe3",
        "outputId": "ad83cbce-ab85-4460-8a64-887fd0707d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading the file...\n",
            "Original dataset:\n",
            "YES comments: 9728\n",
            "NO comments: 1433\n",
            "\n",
            "Balanced dataset:\n",
            "YES comments: 1433\n",
            "NO comments: 1433\n",
            "\n",
            "Balanced dataset saved to 'balanced_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read the CSV file\n",
        "print(\"Reading the file...\")\n",
        "df = pd.read_csv('Final_Data_GPT-Abusive.csv')\n",
        "\n",
        "# Separate YES and NO comments\n",
        "yes_comments = df[df['Related'] == 'Yes']\n",
        "no_comments = df[df['Related'] == 'No']\n",
        "\n",
        "print(f\"Original dataset:\")\n",
        "print(f\"YES comments: {len(yes_comments)}\")\n",
        "print(f\"NO comments: {len(no_comments)}\")\n",
        "\n",
        "# Randomly sample YES comments to match NO comments count\n",
        "sampled_yes = yes_comments.sample(n=len(no_comments), random_state=42)\n",
        "\n",
        "# Combine sampled YES comments with all NO comments\n",
        "balanced_df = pd.concat([sampled_yes, no_comments])\n",
        "\n",
        "# Shuffle the final dataset\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nBalanced dataset:\")\n",
        "print(f\"YES comments: {len(balanced_df[balanced_df['Related'] == 'Yes'])}\")\n",
        "print(f\"NO comments: {len(balanced_df[balanced_df['Related'] == 'No'])}\")\n",
        "\n",
        "# Save to new file\n",
        "balanced_df.to_csv('balanced_dataset.csv', index=False)\n",
        "print(\"\\nBalanced dataset saved to 'balanced_dataset.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wLRDLnuyBauJ",
      "metadata": {
        "id": "wLRDLnuyBauJ"
      },
      "source": [
        "**Title: Comparative Analysis of Machine Learning Models for BinaryClassification**\n",
        "\n",
        "**Methodology:**\n",
        "\n",
        "Six different machine learning models were evaluated using the balanced dataset. Each model was trained using an 80-20 train-test split with a fixed random seed (42). The text data was preprocessed using CountVectorizer with English stop words removed. The following models were evaluated:\n",
        "\n",
        "1. Naive Bayes (MultinomialNB)\n",
        "\n",
        "2. Logistic Regression\n",
        "\n",
        "3. Support Vector Machine (LinearSVC)\n",
        "\n",
        "4. Random Forest\n",
        "\n",
        "5. Decision Tree\n",
        "\n",
        "6. XGBoost\n",
        "\n",
        "**Output:**\n",
        "\n",
        "Performance metrics for each model:\n",
        "\n",
        "**Naive Bayes:**\n",
        "\n",
        "Accuracy: 0.50\n",
        "\n",
        "F1-scores: No (0.47), Yes (0.53)\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Accuracy: 0.45\n",
        "\n",
        "F1-scores: No (0.47), Yes (0.44)\n",
        "\n",
        "**Support Vector Machine:**\n",
        "\n",
        "Accuracy: 0.47\n",
        "\n",
        "F1-scores: No (0.48), Yes (0.44)\n",
        "\n",
        "**Random Forest:**\n",
        "\n",
        "Accuracy: 0.51\n",
        "\n",
        "F1-scores: No (0.54), Yes (0.48)\n",
        "\n",
        "**Decision Tree:**\n",
        "\n",
        "Accuracy: 0.49\n",
        "\n",
        "F1-scores: No (0.51), Yes (0.48)\n",
        "\n",
        "**XGBoost:**\n",
        "\n",
        "Accuracy: 0.46\n",
        "\n",
        "F1-scores: No (0.49), Yes (0.43)\n",
        "\n",
        "Among all models tested, the Random Forest classifier showed the best overall performance with an accuracy of 0.51 and the highest F1-score for the \"No\" class (0.54). However, all models showed relatively similar performance, with accuracies ranging between 0.45 and 0.51, indicating the challenging nature of the classification task."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown to download files from Google Drive\n",
        "!pip install gdown\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPa377qFdEkj",
        "outputId": "142a5b9d-616a-4dba-d86d-f4706c11d074"
      },
      "id": "LPa377qFdEkj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.12.14)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Please Run this Cell First\n",
        "file_id = '19vuDz3MQJjYtdiSS9OtMLIPGQwxbyTgb'\n",
        "!gdown --id {file_id} -O balanced_dataset.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ejo_Hhfd_5z",
        "outputId": "4d0c159c-9b52-4d45-ef59-dc9d2cf23fdc"
      },
      "id": "1Ejo_Hhfd_5z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19vuDz3MQJjYtdiSS9OtMLIPGQwxbyTgb\n",
            "To: /content/balanced_dataset.csv\n",
            "100% 3.70M/3.70M [00:00<00:00, 211MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f49a55b-9814-4b5a-8b68-9d6f72df69d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f49a55b-9814-4b5a-8b68-9d6f72df69d2",
        "outputId": "1d140fda-46e0-4c28-e998-fa1aa36ce786"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Cleaning data...\n",
            "Splitting data into training and testing sets...\n",
            "Converting text to features...\n",
            "Training Naive Bayes model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.50\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.46      0.48      0.47       262\n",
            "         Yes       0.54      0.52      0.53       312\n",
            "\n",
            "    accuracy                           0.50       574\n",
            "   macro avg       0.50      0.50      0.50       574\n",
            "weighted avg       0.50      0.50      0.50       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Original dataset size: 2866\n",
            "Training set size: 2292 comments\n",
            "Testing set size: 574 comments\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# 1. Load the data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "\n",
        "# 2. Remove rows with NaN values\n",
        "print(\"Cleaning data...\")\n",
        "df = df.dropna(subset=['comment_body', 'Related'])  # Remove rows with NaN in these columns\n",
        "\n",
        "# 3. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Your text column\n",
        "y = df['Related']    # Your label column\n",
        "\n",
        "# 4. Split data into training (80%) and testing (20%) sets\n",
        "print(\"Splitting data into training and testing sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Convert text to numerical features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 6. Train the Naive Bayes model\n",
        "print(\"Training Naive Bayes model...\")\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_features, y_train)\n",
        "\n",
        "# 7. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = model.predict(X_test_features)\n",
        "\n",
        "# 8. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# 9. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Original dataset size: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)} comments\")\n",
        "print(f\"Testing set size: {len(X_test)} comments\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5180f2e-aae9-4ea6-bfc9-6ec52d4efede",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5180f2e-aae9-4ea6-bfc9-6ec52d4efede",
        "outputId": "738aa358-da36-4e04-b8a5-131deaceeebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Splitting data...\n",
            "Converting text to features...\n",
            "Training Logistic Regression model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.45\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.42      0.53      0.47       262\n",
            "         Yes       0.50      0.39      0.44       312\n",
            "\n",
            "    accuracy                           0.45       574\n",
            "   macro avg       0.46      0.46      0.45       574\n",
            "weighted avg       0.46      0.45      0.45       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Total comments processed: 2866\n",
            "Training set size: 2292\n",
            "Testing set size: 574\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# 1. Load and clean data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "df = df.dropna(subset=['comment_body', 'Related'])  # Remove rows with NaN\n",
        "\n",
        "# 2. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Text data\n",
        "y = df['Related']    # Labels\n",
        "\n",
        "# 3. Split data 80-20\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Convert text to features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Train Logistic Regression model\n",
        "print(\"Training Logistic Regression model...\")\n",
        "model = LogisticRegression(max_iter=1000)  # Increased iterations for convergence\n",
        "model.fit(X_train_features, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = model.predict(X_test_features)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# 8. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Total comments processed: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4a8a436-3b1d-4726-b8e7-28d6349c0419",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4a8a436-3b1d-4726-b8e7-28d6349c0419",
        "outputId": "12438cd2-7b11-4a88-a71a-35f1f4635d19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Splitting data...\n",
            "Converting text to features...\n",
            "Training SVM model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.47\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.43      0.55      0.48       262\n",
            "         Yes       0.51      0.39      0.44       312\n",
            "\n",
            "    accuracy                           0.47       574\n",
            "   macro avg       0.47      0.47      0.46       574\n",
            "weighted avg       0.47      0.47      0.46       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Total comments processed: 2866\n",
            "Training set size: 2292\n",
            "Testing set size: 574\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# 1. Load and clean data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "df = df.dropna(subset=['comment_body', 'Related'])  # Remove rows with NaN\n",
        "\n",
        "# 2. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Text data\n",
        "y = df['Related']    # Labels\n",
        "\n",
        "\n",
        "# 3. Split data 80-20\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Convert text to features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Train SVM model\n",
        "print(\"Training SVM model...\")\n",
        "svm_model = LinearSVC(random_state=42, max_iter=110000)\n",
        "svm_model.fit(X_train_features, y_train)\n",
        "\n",
        "\n",
        "# 6. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = svm_model.predict(X_test_features)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# 8. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Total comments processed: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e09e8ff-b9d7-47cc-8b5e-b7f78800f34d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e09e8ff-b9d7-47cc-8b5e-b7f78800f34d",
        "outputId": "29dff098-0922-4589-9256-64fd55db5cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Splitting data...\n",
            "Converting text to features...\n",
            "Training Random Forest model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.51\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.47      0.63      0.54       262\n",
            "         Yes       0.57      0.42      0.48       312\n",
            "\n",
            "    accuracy                           0.51       574\n",
            "   macro avg       0.52      0.52      0.51       574\n",
            "weighted avg       0.53      0.51      0.51       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Total comments processed: 2866\n",
            "Training set size: 2292\n",
            "Testing set size: 574\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# 1. Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "\n",
        "# 2. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Text data\n",
        "y = df['Related']      # YES/NO labels\n",
        "\n",
        "# 3. Split data 80-20\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Convert text to features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Train Random Forest model\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_features, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = rf_model.predict(X_test_features)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# 8. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Total comments processed: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8c0ca0b-7a4c-42df-8ae7-368cab841386",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8c0ca0b-7a4c-42df-8ae7-368cab841386",
        "outputId": "b3eae7e5-1948-4ea0-ae8f-6d795117dedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Splitting data...\n",
            "Converting text to features...\n",
            "Training Decision Tree model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.49\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.46      0.58      0.51       262\n",
            "         Yes       0.55      0.42      0.48       312\n",
            "\n",
            "    accuracy                           0.49       574\n",
            "   macro avg       0.50      0.50      0.49       574\n",
            "weighted avg       0.51      0.49      0.49       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Total comments processed: 2866\n",
            "Training set size: 2292\n",
            "Testing set size: 574\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# 1. Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "\n",
        "# 2. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Text data\n",
        "y = df['Related']      # YES/NO labels\n",
        "\n",
        "# 3. Split data 80-20\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 4. Convert text to features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 5. Train Decision Tree model\n",
        "print(\"Training Decision Tree model...\")\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train_features, y_train)\n",
        "\n",
        "# 6. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = dt_model.predict(X_test_features)\n",
        "\n",
        "# 7. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, predictions):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# 8. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Total comments processed: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "354ce6b0-c6b4-424f-89ce-6146d1befc50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "354ce6b0-c6b4-424f-89ce-6146d1befc50",
        "outputId": "8262e8c4-2ad8-4853-c1a9-0087affff1ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Converting labels to numbers...\n",
            "Splitting data...\n",
            "Converting text to features...\n",
            "Training XGBoost model...\n",
            "Making predictions...\n",
            "\n",
            "Model Performance:\n",
            "--------------------------------------------------\n",
            "Accuracy: 0.46\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          No       0.43      0.56      0.49       262\n",
            "         Yes       0.51      0.38      0.43       312\n",
            "\n",
            "    accuracy                           0.46       574\n",
            "   macro avg       0.47      0.47      0.46       574\n",
            "weighted avg       0.47      0.46      0.46       574\n",
            "\n",
            "\n",
            "Dataset Information:\n",
            "Total comments processed: 2866\n",
            "Training set size: 2292\n",
            "Testing set size: 574\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load data\n",
        "print(\"Loading data...\")\n",
        "df = pd.read_csv('balanced_dataset.csv')\n",
        "\n",
        "# 2. Prepare features (X) and labels (y)\n",
        "X = df['comment_body']  # Text data\n",
        "y = df['Related']      # YES/NO labels\n",
        "\n",
        "# 3. Convert YES/NO to 1/0\n",
        "print(\"Converting labels to numbers...\")\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)  # Converts 'YES' to 1, 'NO' to 0\n",
        "\n",
        "# 4. Split data 80-20\n",
        "print(\"Splitting data...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Convert text to features\n",
        "print(\"Converting text to features...\")\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# 6. Train XGBoost model\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "xgb_model.fit(X_train_features, y_train)\n",
        "\n",
        "# 7. Make predictions\n",
        "print(\"Making predictions...\")\n",
        "predictions = xgb_model.predict(X_test_features)\n",
        "\n",
        "# 8. Convert predictions back to YES/NO for the report\n",
        "predictions_labels = label_encoder.inverse_transform(predictions)\n",
        "y_test_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# 9. Print results\n",
        "print(\"\\nModel Performance:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"Accuracy: {accuracy_score(y_test_labels, predictions_labels):.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test_labels, predictions_labels))\n",
        "\n",
        "# 10. Print dataset sizes\n",
        "print(\"\\nDataset Information:\")\n",
        "print(f\"Total comments processed: {len(df)}\")\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Testing set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78b388b2-0f75-4916-b489-761e07af0e68",
      "metadata": {
        "id": "78b388b2-0f75-4916-b489-761e07af0e68"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}